{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492d4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完整词汇表： {'<OOV>': 1, 'i': 2, 'programming': 3, 'is': 4, 'love': 5, 'hate': 6, 'bugs': 7, 'amazing': 8, 'c': 9, 'awesome': 10, 'but': 11, 'prefer': 12, 'python': 13}\n",
      "全部计数结果: OrderedDict([('i', 3), ('love', 1), ('programming', 2), ('hate', 1), ('bugs', 1), ('is', 2), ('amazing', 1), ('c', 1), ('awesome', 1), ('but', 1), ('prefer', 1), ('python', 1)])\n",
      "序列： [[2, 1, 3], [2, 1, 1], [3, 4, 1], [1, 4, 1, 1, 2, 1, 1]]\n",
      "文本： ['i <OOV> programming', 'i <OOV> <OOV>', 'programming is <OOV>', '<OOV> is <OOV> <OOV> i <OOV> <OOV>']\n",
      "词袋化后的结果：\n",
      " [[0 0 0 0 2 1 3]\n",
      " [0 0 0 0 2 1 1]\n",
      " [0 0 0 0 3 4 1]\n",
      " [1 4 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 文本预处理处理教学\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 示例文本\n",
    "texts = [\n",
    "    \"I love programming!\",\n",
    "    \"I hate bugs\",\n",
    "    \"Programming is amazing\",\n",
    "    \"C++ is awesome but I prefer python\",\n",
    "]\n",
    "\n",
    "# 创建分词器，限制词汇表为前 5 个高频单词，选取词汇表中索引号严格小于5的单词组成新的索引表\n",
    "tokenizer = Tokenizer(num_words=5, oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(\n",
    "    texts\n",
    ")  # 构建完整词汇表，出现频率大的数字被分配为更小的索引号，出现频率相同的词，谁先出现谁索引号小，而OOV默认分配为1\n",
    "\n",
    "# 查看完整的词汇表\n",
    "print(\"完整词汇表：\", tokenizer.word_index)\n",
    "print(\"全部计数结果:\", tokenizer.word_counts)\n",
    "\n",
    "# 将文本转为序列\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"序列：\", sequences)\n",
    "\n",
    "# 将序列还原为文本\n",
    "text = tokenizer.sequences_to_texts(sequences)\n",
    "print(\"文本：\", text)\n",
    "\n",
    "# 确保赐予序列的长度一致，便于后续的embedding向量化处理\n",
    "padded = pad_sequences(sequences)  # 可设置 max_len 规定矩阵长度\n",
    "print(\"词袋化后的结果：\\n\", padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbf0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'love', 'nlp'], ['rnn', 'is', 'awesome']]\n",
      "['i', 'love', 'nlp', 'rnn', 'is', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"I love NLP\", \"RNN is awesome\"]\n",
    "\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "print(tokenized_sentences)\n",
    "print(\n",
    "    sum(tokenized_sentences, [])\n",
    ")  # 以空列表为起始值，从tokenized_sentences中逐个取出数字添加\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T12:54:21.128263Z",
     "start_time": "2025-01-02T12:54:21.118804Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转化后的张量为: tensor([[2, 3, 6],\n",
      "        [4, 1, 5]])\n",
      "向量化后结果是:\n",
      " tensor([[[ 0.5419, -0.0278, -0.0237,  0.4598],\n",
      "         [ 0.6662,  0.9331, -0.6065, -1.9131],\n",
      "         [ 1.5078,  0.5253,  0.4281, -0.2084]],\n",
      "\n",
      "        [[-0.4010,  0.4451, -0.2506, -0.2782],\n",
      "         [-0.0503,  2.3237,  0.1001, -0.1376],\n",
      "         [-0.6257, -0.3146, -0.3975, -1.5889]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 示例句子\n",
    "sentences = [\"I love NLP\", \"RNN is awesome\"]\n",
    "\n",
    "# 1. 分词和构建词典\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "vocab = {\n",
    "    word: idx for idx, word in enumerate(set(sum(tokenized_sentences, [])), start=1)\n",
    "}\n",
    "vocab[\"<pad>\"] = 0  # 添加填充符\n",
    "\n",
    "# 2. 转换为索引\n",
    "indexed_sentences = [\n",
    "    [vocab[word] for word in sentence] for sentence in tokenized_sentences\n",
    "]\n",
    "\n",
    "# 3. 填充序列（补齐至最长长度）\n",
    "max_length = max(len(seq) for seq in indexed_sentences)\n",
    "padded_sentences = [\n",
    "    seq + [vocab[\"<pad>\"]] * (max_length - len(seq)) for seq in indexed_sentences\n",
    "]\n",
    "\n",
    "# 转换为张量 (batch_size, seq_length)\n",
    "inputs = torch.tensor(padded_sentences)\n",
    "print(\"转化后的张量为:\", inputs)\n",
    "\n",
    "embed_size = 4  # 嵌入向量的维度\n",
    "embedding = nn.Embedding(\n",
    "    len(vocab), embed_size\n",
    ")  # 第一个参数是告诉Embedding需要为几个单词创建词向量，后续根据每个单词的索引去匹配它们对应的词向量\n",
    "embedded_inputs = embedding(\n",
    "    inputs\n",
    ")  # (batch_size, seq_length, embed_size)  # 对每一个token都根据embed_size进行embedding的操作，转化为纵轴是单词，横轴是向量形式\n",
    "print(\"向量化后结果是:\\n\", embedded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79948d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# 定义单层 RNN\n",
    "hidden_size = 5  # 隐藏状态的维度\n",
    "rnn = nn.RNN(\n",
    "    input_size=embed_size, hidden_size=hidden_size, batch_first=True\n",
    ")  # 形状为 (batch_size, seq_len, input_size)，seq_len表示一句话的时间步数\n",
    "\n",
    "# 初始化隐藏状态 (num_layers=1, batch_size=2, hidden_size=5)，每一层RNN(年级)使用的隐藏层不一样，每个批次(科目)的数据也是不同的隐藏层\n",
    "h0 = torch.zeros(1, inputs.size(0), hidden_size)\n",
    "print(h0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3c85ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出为: tensor([[[-0.4887,  0.4980, -0.0680, -0.0752, -0.3592],\n",
      "         [-0.8153,  0.1253, -0.9064,  0.0532, -0.1003],\n",
      "         [-0.1322,  0.7004, -0.4755, -0.1472, -0.5657]],\n",
      "\n",
      "        [[-0.7067,  0.0172, -0.0317,  0.2895, -0.1139],\n",
      "         [-0.6437, -0.4387, -0.6508,  0.6505, -0.6509],\n",
      "         [-0.8207,  0.3801,  0.2546, -0.3612,  0.3170]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "隐藏层为: tensor([[[-0.1322,  0.7004, -0.4755, -0.1472, -0.5657],\n",
      "         [-0.8207,  0.3801,  0.2546, -0.3612,  0.3170]]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "输入形状: torch.Size([2, 3, 4])\n",
      "输出形状 (output): torch.Size([2, 3, 5])\n",
      "最后隐藏状态 (hn): torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "output, hn = rnn(embedded_inputs, h0)\n",
    "\n",
    "# 打印结果\n",
    "print(\"输出为:\", output)\n",
    "print(\"隐藏层为:\", hn)\n",
    "\n",
    "print(\"输入形状:\", embedded_inputs.shape)  # (batch_size, seq_length, embed_size)\n",
    "print(\"输出形状 (output):\", output.shape)  # (batch_size, seq_length, hidden_size)\n",
    "print(\"最后隐藏状态 (hn):\", hn.shape)  # (num_layers, batch_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5c0459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0312,  0.2926],\n",
      "         [ 0.1681, -0.0372],\n",
      "         [ 0.1057,  0.1251]],\n",
      "\n",
      "        [[-0.1261,  0.3593],\n",
      "         [ 0.0533,  0.1337],\n",
      "         [-0.2788,  0.3478]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "liner = nn.Linear(hidden_size, 2)\n",
    "result = liner(output)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24efd037092a243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T13:40:27.940067Z",
     "start_time": "2025-01-02T13:40:27.922253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output的值是:tensor([[[-0.6493,  0.1630,  0.0098, -0.1999, -0.1375,  0.1564, -0.1979,\n",
      "           0.1462,  0.1414, -0.1055,  0.0123,  0.2600,  0.2173, -0.0925,\n",
      "          -0.3207,  0.1899],\n",
      "         [-0.5886,  0.1960,  0.2016, -0.1176, -0.1485, -0.1782, -0.2423,\n",
      "           0.1649,  0.2419, -0.1088, -0.1592,  0.2728,  0.3155,  0.0618,\n",
      "          -0.5090,  0.2603],\n",
      "         [-0.6759,  0.3456,  0.3108, -0.4451,  0.0052,  0.5187, -0.2579,\n",
      "           0.2178,  0.4540, -0.1011, -0.3092,  0.4792,  0.3551,  0.1834,\n",
      "          -0.2979, -0.2227],\n",
      "         [-0.5810,  0.4812,  0.1900, -0.3839,  0.0947,  0.3237, -0.5739,\n",
      "           0.3276,  0.2959,  0.0030, -0.1377,  0.4063,  0.5563,  0.1789,\n",
      "          -0.2867, -0.2339]],\n",
      "\n",
      "        [[-0.5326,  0.1938,  0.0674, -0.1837, -0.1406,  0.0770, -0.0851,\n",
      "           0.0753, -0.0088, -0.2840, -0.1261,  0.2203,  0.2584, -0.0847,\n",
      "          -0.3063,  0.3108],\n",
      "         [-0.6541,  0.1903,  0.2583, -0.2384, -0.1740,  0.0144, -0.3644,\n",
      "           0.2328,  0.3021, -0.1352, -0.1664,  0.4344,  0.2253,  0.2096,\n",
      "          -0.3569,  0.1010],\n",
      "         [-0.4969,  0.3837,  0.0275, -0.3944,  0.1688,  0.1656, -0.3998,\n",
      "           0.3507,  0.1767,  0.0946, -0.1305,  0.1540,  0.5648,  0.1727,\n",
      "          -0.3046, -0.1249],\n",
      "         [-0.5371,  0.2996,  0.1249, -0.1720,  0.0303,  0.2105, -0.4489,\n",
      "           0.1773,  0.3055,  0.0919, -0.1040,  0.3231,  0.3433,  0.0352,\n",
      "          -0.3648,  0.0929]]], grad_fn=<TransposeBackward1>)\n",
      "hidden的值是:tensor([[[ 0.5436,  0.0876, -0.1223,  0.5729,  0.0089, -0.2702,  0.7464,\n",
      "          -0.4158,  0.1176, -0.3399,  0.4584, -0.6567,  0.0454,  0.0730,\n",
      "           0.5059,  0.2043],\n",
      "         [ 0.6847, -0.0176,  0.4979,  0.0095,  0.0653, -0.5298,  0.5115,\n",
      "          -0.3697,  0.2624,  0.4814,  0.3743, -0.2079,  0.1169,  0.5391,\n",
      "           0.3309,  0.1180]],\n",
      "\n",
      "        [[ 0.6747,  0.1383,  0.2694, -0.5250,  0.4033,  0.0064,  0.0810,\n",
      "           0.6071,  0.3285, -0.0924, -0.4111,  0.4587,  0.3091, -0.0132,\n",
      "          -0.2271, -0.0616],\n",
      "         [ 0.5109, -0.1033, -0.1728, -0.2034,  0.1244,  0.1688,  0.1225,\n",
      "           0.5593, -0.0394, -0.2206, -0.6022,  0.6062,  0.3431,  0.4069,\n",
      "          -0.0208, -0.2851]],\n",
      "\n",
      "        [[-0.5810,  0.4812,  0.1900, -0.3839,  0.0947,  0.3237, -0.5739,\n",
      "           0.3276,  0.2959,  0.0030, -0.1377,  0.4063,  0.5563,  0.1789,\n",
      "          -0.2867, -0.2339],\n",
      "         [-0.5371,  0.2996,  0.1249, -0.1720,  0.0303,  0.2105, -0.4489,\n",
      "           0.1773,  0.3055,  0.0919, -0.1040,  0.3231,  0.3433,  0.0352,\n",
      "          -0.3648,  0.0929]]], grad_fn=<StackBackward0>)\n",
      "模型输出 (分类): tensor([[ 0.0433, -0.1572],\n",
      "        [ 0.0459, -0.2138]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 多层RNN示例代码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 示例句子和标签\n",
    "sentences = [\"I love NLP\", \"This movie is bad\"]\n",
    "labels = torch.tensor([1, 0])  # 最终句子的标签\n",
    "\n",
    "# 分词和词典\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "vocab = {\n",
    "    word: idx for idx, word in enumerate(set(sum(tokenized_sentences, [])), start=1)\n",
    "}\n",
    "vocab[\"<pad>\"] = 0\n",
    "\n",
    "# 转换为索引\n",
    "indexed_sentences = [\n",
    "    [vocab[word] for word in sentence] for sentence in tokenized_sentences\n",
    "]\n",
    "max_length = max(len(seq) for seq in indexed_sentences)\n",
    "padded_sentences = [seq + [0] * (max_length - len(seq)) for seq in indexed_sentences]\n",
    "inputs = torch.tensor(padded_sentences)  # Shape: (batch_size, seq_length)\n",
    "\n",
    "\n",
    "# 定义多层 RNN 模型\n",
    "class MultiLayerRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            embed_size, hidden_size, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # 嵌入层\n",
    "        output, hidden = self.rnn(x)  # RNN 输出\n",
    "        print(f\"output的值是:{output}\")\n",
    "        print(f\"hidden的值是:{hidden}\")\n",
    "        out = self.fc(hidden[-1])  # 取最后一层的隐藏状态作为分类输入\n",
    "        return out\n",
    "\n",
    "\n",
    "# 模型初始化\n",
    "vocab_size = len(vocab)  # 8\n",
    "embed_size = 8\n",
    "hidden_size = 16\n",
    "num_classes = 2\n",
    "num_layers = 3  # 三层 RNN\n",
    "\n",
    "model = MultiLayerRNN(vocab_size, embed_size, hidden_size, num_classes, num_layers)\n",
    "\n",
    "# 前向传播\n",
    "outputs = model(inputs)\n",
    "print(\"模型输出 (分类):\", outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6677eb711f089a39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:44:02.758345Z",
     "start_time": "2025-01-03T00:44:02.746819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token化后的形式为:\n",
      " [['i', 'love', 'nlp'], ['this', 'movie', 'is', 'bad'], ['i', 'hate', 'talking']]\n",
      "字符字典为:\n",
      " {'love': 1, 'i': 2, 'nlp': 3, 'talking': 4, 'movie': 5, 'bad': 6, 'hate': 7, 'this': 8, 'is': 9, '<pad>': 0}\n",
      "转化为索引矩阵:\n",
      " [[2, 1, 3], [8, 5, 9, 6], [2, 7, 4]]\n",
      "输入为:\n",
      " tensor([[2, 1, 3, 0],\n",
      "        [8, 5, 9, 6],\n",
      "        [2, 7, 4, 0]])\n",
      "embedding化后的结果是:\n",
      " tensor([[[ 0.1061,  0.8229,  0.4422, -0.1745],\n",
      "         [ 0.8715,  0.1458, -0.8946,  0.6747],\n",
      "         [-1.2595,  1.6699, -0.1777,  2.6016],\n",
      "         [ 0.8748, -0.6086, -0.2546,  1.2928]],\n",
      "\n",
      "        [[ 0.3344,  0.5769,  0.2866, -1.2785],\n",
      "         [ 0.0943, -0.7924,  1.3047, -0.7229],\n",
      "         [ 0.4095,  1.9974, -0.4329, -0.8676],\n",
      "         [-0.4056,  0.8265, -1.7672, -0.5232]],\n",
      "\n",
      "        [[ 0.1061,  0.8229,  0.4422, -0.1745],\n",
      "         [ 0.8379, -1.0641, -0.0122, -0.8653],\n",
      "         [ 1.1144,  1.1977, -0.4135,  1.0867],\n",
      "         [ 0.8748, -0.6086, -0.2546,  1.2928]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 示例句子\n",
    "sentences = [\"I love NLP\", \"This movie is bad\", \"I hate talking\"]\n",
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
    "vocab = {\n",
    "    word: idx for idx, word in enumerate(set(sum(tokenized_sentences, [])), start=1)\n",
    "}\n",
    "vocab[\"<pad>\"] = 0\n",
    "\n",
    "# 转换为索引\n",
    "indexed_sentences = [\n",
    "    [vocab[word] for word in sentence] for sentence in tokenized_sentences\n",
    "]\n",
    "max_length = max(\n",
    "    len(seq) for seq in indexed_sentences\n",
    ")  # 求出列表最大长度->进行填补操作\n",
    "padded_sentences = [\n",
    "    seq + [0] * (max_length - len(seq)) for seq in indexed_sentences\n",
    "]  # 使用填充法，将全部的seq_length转化为一致\n",
    "inputs = torch.tensor(padded_sentences)  # Shape: (batch_size, seq_length)\n",
    "\n",
    "print(\"Token化后的形式为:\\n\", tokenized_sentences)\n",
    "print(\"字符字典为:\\n\", vocab)\n",
    "print(\"转化为索引矩阵:\\n\", indexed_sentences)\n",
    "print(\"输入为:\\n\", inputs)\n",
    "\n",
    "# 嵌入层\n",
    "embed_size = 4  # 嵌入向量的维度\n",
    "embedding = nn.Embedding(len(vocab), embed_size)\n",
    "embedded_inputs = embedding(inputs)  # (batch_size, seq_length, embed_size)\n",
    "print(\"embedding化后的结果是:\\n\", embedded_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
