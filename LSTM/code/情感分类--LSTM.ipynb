{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必须包\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ItemID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Sentiment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SentimentSource",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SentimentText",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "fbe8a237-10e2-4a99-8cad-a4c4e260bb26",
       "rows": [
        [
         "0",
         "1",
         "0",
         "Sentiment140",
         "                     is so sad for my APL friend............."
        ],
        [
         "1",
         "2",
         "0",
         "Sentiment140",
         "                   I missed the New Moon trailer..."
        ],
        [
         "2",
         "3",
         "1",
         "Sentiment140",
         "              omg its already 7:30 :O"
        ],
        [
         "3",
         "4",
         "0",
         "Sentiment140",
         "          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)..."
        ],
        [
         "4",
         "5",
         "0",
         "Sentiment140",
         "         i think mi bf is cheating on me!!!       T_T"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入分析数据\n",
    "data = pd.read_csv(\n",
    "    r\"../data/Sentiment Analysis Dataset.csv\",\n",
    "    encoding=\"UTF-8\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    nrows=1000,\n",
    ")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         is so sad for my APL frie...\n",
       "1                       I missed the New Moon trail...\n",
       "2                              omg its already 7:30 :O\n",
       "3              .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4             i think mi bf is cheating on me!!!   ...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取评论信息\n",
    "sentimentText = data[\"SentimentText\"]\n",
    "sentimentText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "# 获取真实的标签\n",
    "labels = torch.tensor(data[\"Sentiment\"])\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 清理数据\n",
    "full_data = sentimentText.to_list()\n",
    "cleaned_data = []\n",
    "\n",
    "for sentence in full_data:\n",
    "    sentence = str(sentence)\n",
    "    # 去除双引号与单引号\n",
    "    sentence = sentence.replace(\"'\", \"\")\n",
    "    sentence = sentence.replace('\"', \"\")\n",
    "    # 去除空白\n",
    "    sentence = sentence.strip()\n",
    "    cleaned_data.append(sentence)\n",
    "\n",
    "type(cleaned_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取评论的词嵌入数据\n",
    "def get_embeddingData(cleaned_data, dimension=256):\n",
    "    client = OpenAI()\n",
    "    batch_size = 100\n",
    "    # 使用循环提取向量化后数据\n",
    "    embedded_text = []\n",
    "\n",
    "    for i in range(len(cleaned_data) // batch_size):\n",
    "        batch = cleaned_data[i * batch_size : (i + 1) * batch_size]\n",
    "        try:\n",
    "            full_data = client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=batch,\n",
    "                dimensions=dimension,\n",
    "                encoding_format=\"float\",\n",
    "            )\n",
    "\n",
    "            # 提取嵌入向量\n",
    "            count = 0\n",
    "            while count < batch_size:\n",
    "                embedded_text.append(full_data.data[count].embedding)\n",
    "                count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理第{i}批的时出错:{str(e)}\")\n",
    "            break\n",
    "    return embedded_text\n",
    "\n",
    "\n",
    "# 查看数据维度\n",
    "embedded_text = get_embeddingData(cleaned_data=cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据供以后使用\n",
    "\n",
    "# 转换为numpy数组并保存\n",
    "embedded_array = np.array(embedded_text)\n",
    "np.save(\"embedded_vectors.npy\", embedded_array)\n",
    "\n",
    "# 之后可以这样加载\n",
    "# loaded_vectors = np.load('embedded_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_array = np.load(\"embedded_vectors.npy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 256)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# 检查数据维度\n",
    "print(embedded_array.shape)\n",
    "\n",
    "# 转化为tensor\n",
    "input_data = torch.tensor(embedded_array)\n",
    "input_data = input_data.to(torch.float32)\n",
    "print(type(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class advancedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layer, drop_out=0.5):\n",
    "        super().__init__()\n",
    "        # LSTM层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layer,\n",
    "            batch_first=True,\n",
    "            dropout=drop_out if num_layer > 1 else 0,  # 作用于层与层之间的连接\n",
    "        )\n",
    "\n",
    "        # 批标准化层\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # Dropout层 - 使得输出不过多依赖某一个参数\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # 分类层\n",
    "        self.classifier = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, origin_data):\n",
    "        # 调用LSTM层\n",
    "        result, (h_n, c_n) = self.lstm(\n",
    "            origin_data\n",
    "        )  # h_n的形状是: (num_layer, batch_size, hidden_size)\n",
    "        last_hidden = h_n[-1]\n",
    "\n",
    "        # 批归一化\n",
    "        x = self.batch_norm(last_hidden)\n",
    "\n",
    "        # Droupout层\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 分类层\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "input_size = 256\n",
    "hidden_size = 512\n",
    "output_size = 2\n",
    "num_layer = 3\n",
    "\n",
    "# 调用模型获得结果\n",
    "lstm = advancedLSTM(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    num_layer=num_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 256])\n",
      "X_train shape: torch.Size([800, 1, 256])\n",
      "y_train shape: torch.Size([800])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 分割数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_data, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 将维度升高一维便于LSTM处理\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)\n",
    "\n",
    "# 查看数据维度\n",
    "print(X_test.shape)  # (num, seq_len, input_size)\n",
    "\n",
    "# 检查输入维度\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练参数\n",
    "n_epochs = 50\n",
    "learing_rate = 0.001\n",
    "\n",
    "# 定义损失函数于优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.6034\n",
      "Test Accuracy: 68.50%\n",
      "Epoch [10/50], Loss: 0.5120\n",
      "Test Accuracy: 68.50%\n",
      "Epoch [15/50], Loss: 0.4817\n",
      "Test Accuracy: 68.50%\n",
      "Epoch [20/50], Loss: 0.4732\n",
      "Test Accuracy: 69.00%\n",
      "Epoch [25/50], Loss: 0.4571\n",
      "Test Accuracy: 72.00%\n",
      "Epoch [30/50], Loss: 0.4427\n",
      "Test Accuracy: 75.00%\n",
      "Epoch [35/50], Loss: 0.4359\n",
      "Test Accuracy: 80.00%\n",
      "Epoch [40/50], Loss: 0.4306\n",
      "Test Accuracy: 80.50%\n",
      "Epoch [45/50], Loss: 0.4285\n",
      "Test Accuracy: 83.00%\n",
      "Epoch [50/50], Loss: 0.4153\n",
      "Test Accuracy: 83.00%\n"
     ]
    }
   ],
   "source": [
    "# 循环训练\n",
    "for epoch in range(n_epochs):\n",
    "    lstm.train()  # 转化为训练模式\n",
    "\n",
    "    # 前向传播\n",
    "    outputs = lstm(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 每五个训练次数打印一次准确率\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 评估模式\n",
    "        lstm.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = lstm(X_test)\n",
    "            _, predicted = torch.max(test_outputs.data, 1)\n",
    "            accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "            print(f\"Test Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最终测试集准确率: 83.00%\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    # 测试集预测\n",
    "    test_outputs = lstm(X_test)\n",
    "    _, predicted = torch.max(\n",
    "        test_outputs.data, 1\n",
    "    )  # 找到最大值与最大值的索引位置(argmax函数)\n",
    "\n",
    "    # 计算准确率\n",
    "    total = y_test.size(0)\n",
    "    correct = (\n",
    "        (predicted == y_test).sum().item()\n",
    "    )  # 使用item()转化为python的数据类型，减少内存\n",
    "\n",
    "    print(f\"\\n最终测试集准确率: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 形状问题\n",
    "\n",
    "- output 显示的是最后一层 LSTM 中每一个 batch 的每一个时间步的输出；\n",
    "- h_n 显示的是 LSTM 中全部层数中每一个 batch 最后一个时间步的输出；\n",
    "- c_n 显示的是 LSTM 中全部层数中每一个 batch 最后一个时间步的 cell state；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 512])\n",
      "**********\n",
      "torch.Size([3, 200, 512])\n",
      "**********\n",
      "torch.Size([3, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "# 形状确定\n",
    "# * 确定LSTM在多层情况下的返回值\n",
    "output = lstm.lstm(X_test)\n",
    "\n",
    "print(\n",
    "    output[0].shape, end=\"\\n\" + \"*\" * 10 + \"\\n\"\n",
    ")  # (batch_size, seq_len, hidden_size))\n",
    "\n",
    "print(\n",
    "    output[1][0].shape, end=\"\\n\" + \"*\" * 10 + \"\\n\"\n",
    ")  # (num_layer, batch_size, hidden_size)\n",
    "\n",
    "print(output[1][1].shape)  # (num_layer, batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整封装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, input_size=1536, hidden_size=512, output_size=2, num_layers=3):\n",
    "        \"\"\"初始化情感分析器\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.model = None\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def _init_model(self):\n",
    "        \"\"\"初始化LSTM模型\"\"\"\n",
    "        self.model = advancedLSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            output_size=self.output_size,\n",
    "            num_layer=self.num_layers,\n",
    "        )\n",
    "\n",
    "    def vectorize_data(self, cleaned_data, dimension=1536, batch_size=100):\n",
    "        \"\"\"数据向量化处理\"\"\"\n",
    "        embedded_text = []\n",
    "\n",
    "        for i in range(len(cleaned_data) // batch_size):\n",
    "            batch = cleaned_data[i * batch_size : (i + 1) * batch_size]\n",
    "            try:\n",
    "                full_data = self.client.embeddings.create(\n",
    "                    model=\"text-embedding-3-small\",\n",
    "                    input=batch,\n",
    "                    dimensions=dimension,\n",
    "                    encoding_format=\"float\",\n",
    "                )\n",
    "\n",
    "                for count in range(batch_size):\n",
    "                    embedded_text.append(full_data.data[count].embedding)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理第{i}批时出错:{str(e)}\")\n",
    "                break\n",
    "\n",
    "        return torch.tensor(np.array(embedded_text), dtype=torch.float32)\n",
    "\n",
    "    def train(\n",
    "        self, input_data, labels, n_epochs=50, learning_rate=0.001, test_size=0.2\n",
    "    ):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        # 初始化模型\n",
    "        if self.model is None:\n",
    "            self._init_model()\n",
    "\n",
    "        # 数据集分割\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            input_data, labels, test_size=test_size, random_state=42\n",
    "        )\n",
    "\n",
    "        # 添加序列维度\n",
    "        X_train = X_train.unsqueeze(1)\n",
    "        X_test = X_test.unsqueeze(1)\n",
    "\n",
    "        # 定义损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # 训练循环\n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Training\"):\n",
    "            self.model.train()\n",
    "\n",
    "            outputs = self.model(X_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "                # 评估\n",
    "                accuracy = self.evaluate(X_test, y_test)\n",
    "                print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "        return self.evaluate(X_test, y_test)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"评估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = self.model(X_test)\n",
    "            _, predicted = torch.max(test_outputs.data, 1)\n",
    "            accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"预测单个文本的情感\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"模型未训练，请先训练模型\")\n",
    "        # 添加数据类型检查\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        elif not isinstance(text, list):\n",
    "            raise ValueError(\"输入必须是字符串或字符串列表\")\n",
    "\n",
    "        # 获取文本嵌入\n",
    "        embedding = self.vectorize_data(text, len(text))\n",
    "        embedding = embedding.unsqueeze(1)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(embedding)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        result = [\n",
    "            \"积极消息\" if single_result == 1 else \"消极消息\"\n",
    "            for single_result in predicted\n",
    "        ]\n",
    "        return result[0] if isinstance(text, str) else result\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        if self.model is not None:\n",
    "            torch.save(self.model.state_dict(), path)\n",
    "            print(f\"模型已保存至: {path}\")\n",
    "        else:\n",
    "            print(\"没有可保存的模型\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        self._init_model()\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        print(f\"模型已从 {path} 加载\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 256, got 1536",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m vectors \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mvectorize_data(cleaned_data)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m final_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m最终准确率: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 预测新文本\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 75\u001b[0m, in \u001b[0;36mSentimentAnalyzer.train\u001b[1;34m(self, input_data, labels, n_epochs, learning_rate, test_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 75\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[0;32m     78\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\34408\\.conda\\envs\\hug_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\34408\\.conda\\envs\\hug_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 24\u001b[0m, in \u001b[0;36madvancedLSTM.forward\u001b[1;34m(self, origin_data)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, origin_data):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# 调用LSTM层\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     result, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin_data\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# h_n的形状是: (num_layer, batch_size, hidden_size)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     last_hidden \u001b[38;5;241m=\u001b[39m h_n[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# 批归一化\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\34408\\.conda\\envs\\hug_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\34408\\.conda\\envs\\hug_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\34408\\.conda\\envs\\hug_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1100\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m   1094\u001b[0m         max_batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1097\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m   1098\u001b[0m     )\n\u001b[0;32m   1099\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[1;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32mc:\\Users\\34408\\.conda\\envs\\hug_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1000\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    997\u001b[0m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    998\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[0;32m    999\u001b[0m ):\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1002\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[0;32m   1007\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1010\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\34408\\.conda\\envs\\hug_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:312\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m     )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 256, got 1536"
     ]
    }
   ],
   "source": [
    "# 创建情感分析器实例\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# 数据预处理和向量化\n",
    "vectors = analyzer.vectorize_data(cleaned_data)\n",
    "\n",
    "# 训练模型\n",
    "final_accuracy = analyzer.train(vectors, labels)\n",
    "print(f\"最终准确率: {final_accuracy:.2%}\")\n",
    "\n",
    "# 预测新文本\n",
    "text = [\"I love this movie!\", \"I hate that bitch\", \"Shit!\"]\n",
    "sentiment = analyzer.predict(text)\n",
    "print(f\"文本情感: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排序后的数组: [5, 6, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "def insertion_sort(arr):\n",
    "    # 遍历数组中的每一个元素\n",
    "    for i in range(1, len(arr)):\n",
    "        key = arr[i]\n",
    "        j = i - 1\n",
    "        # 将当前元素插入到已排序部分的正确位置\n",
    "        while j >= 0 and key < arr[j]:\n",
    "            arr[j + 1] = arr[j]\n",
    "            j -= 1\n",
    "        arr[j + 1] = key\n",
    "    return arr\n",
    "\n",
    "# 测试插入排序算法\n",
    "sample_array = [12, 11, 13, 5, 6]\n",
    "sorted_array = insertion_sort(sample_array)\n",
    "print(\"Sorted array is:\", sorted_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
