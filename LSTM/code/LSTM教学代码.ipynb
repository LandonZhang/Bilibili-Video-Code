{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0: torch.Size([256, 8])\n",
      "weight_hh_l0: torch.Size([256, 64])\n",
      "bias_ih_l0: torch.Size([256])\n",
      "bias_hh_l0: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# LSTM结构掌握\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义 LSTM\n",
    "lstm = nn.LSTM(input_size=8, hidden_size=64, num_layers=1, batch_first=True)\n",
    "\n",
    "# 随机输入 (batch_size=32, seq_len=10, input_dim=8)\n",
    "x = torch.randn(32, 10, 8)\n",
    "output, (h_n, c_n) = lstm(x)\n",
    "\n",
    "# 打印参数形状\n",
    "for name, param in lstm.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_t (最终隐藏状态): [[ 0.32201566 -0.1219501 ]\n",
      " [ 0.1272508  -0.10326343]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# LSTM 数学示例（batch_first=True）\n",
    "# ============================\n",
    "\n",
    "# 输入数据和初始隐藏状态\n",
    "# 假设 batch_size = 2，seq_len = 3，input_size = 3\n",
    "x_t = np.array(\n",
    "    [\n",
    "        [[0.5, 0.1, -0.3], [0.3, 0.4, 0.8], [-0.2, 0.6, 0.7]],  # 第一个样本 (3, 3)\n",
    "        [[-0.1, 0.2, 0.5], [0.4, -0.3, 0.9], [0.6, 0.1, -0.4]],  # 第二个样本 (3, 3)\n",
    "    ]\n",
    ")  # 形状 (batch_size=2, seq_len=3, input_size=3)\n",
    "\n",
    "# 初始隐藏状态和细胞状态\n",
    "h_prev = np.zeros((2, 2))  # (batch_size=2, hidden_size=2)\n",
    "c_prev = np.zeros((2, 2))  # (batch_size=2, hidden_size=2)\n",
    "\n",
    "# 拼接后的权重和偏置\n",
    "W_ih = np.random.randn(\n",
    "    8, 3\n",
    ")  # 输入到隐藏层的权重 (4 * hidden_size, input_size=3) => (8, 3)\n",
    "W_hh = np.random.randn(\n",
    "    8, 2\n",
    ")  # 隐藏状态到隐藏层的权重 (4 * hidden_size, hidden_size=2) => (8, 2)\n",
    "b = np.random.randn(8, 1)  # 偏置项 (4 * hidden_size, 1) => (8, 1)\n",
    "\n",
    "# 遍历每个时间步\n",
    "for t in range(3):  # seq_len = 3\n",
    "    x_step = x_t[:, t, : ]  # 当前时间步的输入数据，形状 (batch_size, input_size) => (2, 3)\n",
    "\n",
    "    # Step 1: 计算线性变换结果 Z（未激活的门值）\n",
    "    z = np.dot(x_step, W_ih.T) + np.dot(h_prev, W_hh.T) + b.T  # 形状 (batch_size, 8)\n",
    "\n",
    "    # Step 2: 切分 Z，分别计算四个门的激活值\n",
    "    f_t = 1 / (1 + np.exp(-z[:, :2]))  # 遗忘门 (batch_size, 2)\n",
    "    i_t = 1 / (1 + np.exp(-z[:, 2:4]))  # 输入门 (batch_size, 2)\n",
    "    c_tilde = np.tanh(z[:, 4:6])  # 候选状态 (batch_size, 2)\n",
    "    o_t = 1 / (1 + np.exp(-z[:, 6:]))  # 输出门 (batch_size, 2)\n",
    "\n",
    "    # Step 3: 更新细胞状态 c_t\n",
    "    c_prev = f_t * c_prev + i_t * c_tilde  # 形状 (batch_size, 2)\n",
    "\n",
    "    # Step 4: 计算当前时间步的隐藏状态 h_t\n",
    "    h_prev = o_t * np.tanh(c_prev)  # 形状 (batch_size, 2)\n",
    "\n",
    "# 输出每个样本的最终隐藏状态\n",
    "print(\"h_t (最终隐藏状态):\", h_prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础流程 LSTM 输出： (array([[-0.08845272, -0.32381897, -0.28599911,  0.03722394],\n",
      "       [-0.26984503,  0.14548591,  0.11242097, -0.24239174]]), array([[-0.15433578, -0.74551582, -0.36123616,  0.14815187],\n",
      "       [-0.40645539,  0.33771736,  0.17450061, -0.60444706]]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ============================\n",
    "# 基础流程实现 LSTM (加入 batch_size，batch_first=True)\n",
    "# ============================\n",
    "\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # 初始化权重和偏置 (4 个门拼接后的矩阵)\n",
    "        self.W = np.random.randn(4 * hidden_size, input_size)  # (4h, n)\n",
    "        self.U = np.random.randn(4 * hidden_size, hidden_size)  # (4h, h)\n",
    "        self.b = np.zeros((4 * hidden_size, 1))  # (4h, 1)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def forward(self, x_seq):  # x_seq 形状: (batch_size, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = x_seq.shape\n",
    "\n",
    "        # 初始化隐藏状态和细胞状态，形状为 (batch_size, hidden_size)\n",
    "        h, c = np.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "        for t in range(seq_len):  # 遍历时间步\n",
    "            x = x_seq[:, t, :]  # 取出当前时间步的所有 batch 数据，形状 (batch_size, input_size)\n",
    "\n",
    "            # 线性变换 (batch_size, 4h)\n",
    "            z = np.dot(x, self.W.T) + np.dot(h, self.U.T) + self.b.T\n",
    "\n",
    "            # 切分出四个门 (每个门维度为 (batch_size, hidden_size))\n",
    "            f = self.sigmoid(z[:, :self.hidden_size])               # 遗忘门 (batch_size, hidden_size)\n",
    "            i = self.sigmoid(z[:, self.hidden_size:2*self.hidden_size])  # 输入门 (batch_size, hidden_size)\n",
    "            c_tilde = self.tanh(z[:, 2*self.hidden_size:3*self.hidden_size])  # 候选状态 (batch_size, hidden_size)\n",
    "            o = self.sigmoid(z[:, 3*self.hidden_size:])            # 输出门 (batch_size, hidden_size)\n",
    "\n",
    "            # 更新细胞状态和隐藏状态\n",
    "            c = f * c + i * c_tilde  # 细胞状态 (batch_size, hidden_size)\n",
    "            h = o * self.tanh(c)     # 隐藏状态 (batch_size, hidden_size)\n",
    "\n",
    "        return h, c  # 返回最终的隐藏状态与记忆神经元 (batch_size, hidden_size)\n",
    "\n",
    "# 输入数据 (batch_size=2, seq_len=3, input_size=2)\n",
    "x_seq = np.array([\n",
    "    [[0.5, -0.2], [0.1, 0.4], [-0.3, 0.8]],  # 第一个样本\n",
    "    [[-0.1, 0.3], [0.4, -0.5], [0.6, 0.2]]   # 第二个样本\n",
    "])\n",
    "\n",
    "lstm_basic = BasicLSTM(input_size=2, hidden_size=4)\n",
    "output_basic = lstm_basic.forward(x_seq)\n",
    "print(\"基础流程 LSTM 输出：\", output_basic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM输出: tensor([[[-0.1034, -0.0592, -0.0791, -0.1689],\n",
      "         [-0.1429, -0.0698, -0.0909, -0.2653],\n",
      "         [-0.1578, -0.0617, -0.0863, -0.3140]],\n",
      "\n",
      "        [[-0.1025, -0.0597, -0.0848, -0.1668],\n",
      "         [-0.1413, -0.0693, -0.0978, -0.2631],\n",
      "         [-0.1555, -0.0604, -0.0934, -0.3119]]], grad_fn=<TransposeBackward0>)\n",
      "隐藏层数值是: tensor([[[-0.1578, -0.0617, -0.0863, -0.3140],\n",
      "         [-0.1555, -0.0604, -0.0934, -0.3119]]], grad_fn=<StackBackward0>)\n",
      "记忆神经元数值是: tensor([[[-0.5589, -0.1285, -0.2860, -0.8364],\n",
      "         [-0.5630, -0.1262, -0.3085, -0.8338]]], grad_fn=<StackBackward0>)\n",
      "高级 API LSTM 输出： [0.34141046 0.34165925]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 高级 API (PyTorch) 实现 LSTM\n",
    "# ============================\n",
    "\n",
    "\n",
    "class HighLevelLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(HighLevelLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, c_n) = self.lstm(x)  # LSTM 输出 + 最后的隐藏状态和细胞状态\n",
    "        print(\"LSTM输出:\", out)\n",
    "        out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出进行全连接\n",
    "        print(\"隐藏层数值是:\", h_n)\n",
    "        print(\"记忆神经元数值是:\", c_n)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 输入数据（batch_size=1, seq_len=3, input_size=2）\n",
    "x_torch = torch.tensor(\n",
    "    [[[0.5, -0.2], [0.1, 0.4], [-0.3, 0.8]], [[0.6, -0.1], [0.2, 0.5], [-0.2, 0.9]]],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "lstm_high = HighLevelLSTM(input_size=2, hidden_size=4, output_size=1)\n",
    "output_high = lstm_high(x_torch)\n",
    "print(\"高级 API LSTM 输出：\", output_high.detach().numpy().flatten())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
